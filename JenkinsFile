pipeline {
    agent any
    environment {
        LLAMA_DIR = "${WORKSPACE}/llama.cpp"
        MODEL_DIR = "${WORKSPACE}/models"
        MODEL_FILE = "Meta-Llama-3-3B-Instruct.Q4_K_M.gguf"
        MODEL_SOURCE_PATH = "/path/to/your/model/Meta-Llama-3-3B-Instruct.Q4_K_M.gguf" // <-- CHANGE THIS
    }
    stages {
        stage('Clone llama.cpp') {
            steps {
                sh '''
                    if [ ! -d "$LLAMA_DIR" ]; then
                        echo "Cloning llama.cpp..."
                        git clone https://github.com/ggml-org/llama.cpp "$LLAMA_DIR"
                    fi
                '''
            }
        }
        stage('Build llama.cpp') {
            steps {
                sh '''
                    echo "Building llama.cpp..."
                    cd "$LLAMA_DIR"
                    make
                '''
            }
        }
        stage('Copy Model') {
            steps {
                sh '''
                    mkdir -p "$MODEL_DIR"
                    if [ ! -f "$MODEL_DIR/$MODEL_FILE" ]; then
                        echo "Copying model file..."
                        cp "$MODEL_SOURCE_PATH" "$MODEL_DIR/"
                    fi
                '''
            }
        }
        // Optional: Start the server (usually not done in CI)
        // stage('Start Server') {
        //     steps {
        //         sh '''
        //             echo "Starting llama.cpp server..."
        //             "$LLAMA_DIR/server" -m "$MODEL_DIR/$MODEL_FILE" --port 8080
        //         '''
        //     }
        // }
    }
}
